{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForSequenceClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "\n",
    "train_data_full = pd.read_csv('captions_train - Sheet1-3.csv',header=None,names=[\"caption\",\"result\"])\n",
    "test_data = pd.read_csv('captions_test - Sheet1-4.csv',header=None,names=[\"caption\",\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_full\n",
    "print(f\"\\nTraining on {len(train_data)} examples\\n\")\n",
    "\n",
    "print(train_data.sort_values(by=[\"result\"]).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_results = 3\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large', num_labels=num_results)\n",
    "mode = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert result column to one-hot encoding\n",
    "one_hot_train = pd.get_dummies(train_data['result'])\n",
    "one_hot_test = pd.get_dummies(test_data['result'])\n",
    "\n",
    "# Tokenize captions and convert to PyTorch dataset\n",
    "inputs_train = tokenizer(list(train_data['caption']), return_tensors='pt', padding=True)\n",
    "labels_train = torch.tensor(one_hot_train.values, dtype=torch.float32)\n",
    "dataset_train = torch.utils.data.TensorDataset(inputs_train['input_ids'], inputs_train['attention_mask'], labels_train)\n",
    "inputs_test = tokenizer(list(test_data['caption']), return_tensors='pt', padding=True)\n",
    "labels_test = torch.tensor(one_hot_test.values, dtype=torch.float32)\n",
    "dataset_test = torch.utils.data.TensorDataset(inputs_test['input_ids'], inputs_test['attention_mask'], labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def graphLoss(epoch_counter, train_loss_hist, test_loss_hist, loss_name=\"Loss\", start = 0):\n",
    "  fig = plt.figure()\n",
    "  plt.plot(epoch_counter[start:], train_loss_hist[start:], color='blue')\n",
    "  plt.plot(epoch_counter[start:], test_loss_hist[start:], color='red')\n",
    "  plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "  plt.xlabel('#Epochs')\n",
    "  plt.ylabel(loss_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logResults(epoch, num_epochs, train_loss, train_loss_history, test_loss, test_loss_history, epoch_counter, print_interval=1000):\n",
    "  if (epoch%print_interval == 0):  print('Epoch [%d/%d], Train Loss: %.4f, Test Loss: %.4f' %(epoch+1, num_epochs, train_loss, test_loss))\n",
    "  train_loss_history.append(train_loss)\n",
    "  test_loss_history.append(test_loss)\n",
    "  epoch_counter.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Train model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "test_loss_history = []\n",
    "train_loss_history = []\n",
    "epoch_counter = []\n",
    "\n",
    "print(f\"\\nTraining on {len(train_data)} examples\\n\")\n",
    "print(\"Num. Parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Compute average loss after 100 steps\n",
    "    avg_loss = 0\n",
    "    for step, batch in enumerate(data_loader_train):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        avg_loss += loss.item()\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}/{len(data_loader_train)} Loss {loss} Avg Train Loss {avg_loss / (step + 1)}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss = avg_loss / len(data_loader_train)\n",
    "    # Print loss after every epoch\n",
    "    print(f\"Epoch {epoch+1} Test Loss {loss}\")\n",
    "    # Compute accuracy after every epoch\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for step, batch in enumerate(data_loader_test):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predicted = torch.argmax(outputs[0], dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "    print(f\"Test Accuracy {100*correct/total}%\\n\")\n",
    "    logResults(epoch, epochs, avg_loss, train_loss_history, loss, test_loss_history, epoch_counter, 1)\n",
    "\n",
    "graphLoss(epoch_counter, train_loss_history, test_loss_history)\n",
    "# Save model\n",
    "model.save_pretrained('fine-tuned-bart_captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "for step, batch in enumerate(data_loader_test):\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predicted = torch.argmax(outputs[0], dim=1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "print(f\"Accuracy {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = [\"Unfortunate outcome.\", \"HUGE COMEBACK TONIGHT\", \"Final in Dallas.\"]\n",
    "inputs = tokenizer(new_names, return_tensors='pt', padding=True)\n",
    "outputs = model(inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device))\n",
    "predicted = torch.argmax(outputs[0], dim=1)\n",
    "for i in range(len(new_names)):\n",
    "  print(f\"{new_names[i]}: {one_hot_train.columns[predicted[i].item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrix\n",
    "confusion_matrix = torch.zeros(len(one_hot_test.columns), len(one_hot_test.columns))\n",
    "for step, batch in enumerate(data_loader_test):\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    predicted = torch.argmax(outputs[0], dim=1)\n",
    "    for i in range(len(predicted)):\n",
    "        confusion_matrix[torch.argmax(labels[i])][predicted[i]] += 1\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r', xticklabels=one_hot_train.columns, yticklabels=one_hot_train.columns)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
